{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_participant_data(participant_data):\n",
    "    camel_scores = [0] * 5\n",
    "    camel_times = [0] * 5\n",
    "    kebab_scores = [0] * 5\n",
    "    kebab_times = [0] * 5\n",
    "\n",
    "    question_data_dict = participant_data[\"questionData\"]\n",
    "\n",
    "    for _, question_data in question_data_dict.items():\n",
    "        case_format = question_data[\"caseFormat\"]\n",
    "        is_correct = 1 if question_data[\"isCorrect\"] else 0\n",
    "        time_taken = question_data[\"timeTaken\"]\n",
    "\n",
    "        if case_format.startswith(\"camel\"):\n",
    "            camel_scores[int(case_format[-1]) - 1] = is_correct\n",
    "            camel_times[int(case_format[-1]) - 1] = time_taken\n",
    "        elif case_format.startswith(\"kebab\"):\n",
    "            kebab_scores[int(case_format[-1]) - 1] = is_correct\n",
    "            kebab_times[int(case_format[-1]) - 1] = time_taken\n",
    "\n",
    "    camel_average = sum(camel_times) / len(camel_times) if any(camel_times) else 0\n",
    "    kebab_average = sum(kebab_times) / len(kebab_times) if any(kebab_times) else 0\n",
    "\n",
    "    return {\n",
    "        \"participantID\": participant_data[\"participantID\"],\n",
    "        \"experience\": participant_data[\"demographicsAnswers\"][0][\"experience\"],\n",
    "        \"camel1_score\": camel_scores[0],\n",
    "        \"camel2_score\": camel_scores[1],\n",
    "        \"camel3_score\": camel_scores[2],\n",
    "        \"camel4_score\": camel_scores[3],\n",
    "        \"camel5_score\": camel_scores[4],\n",
    "        \"camel1_time\": camel_times[0],\n",
    "        \"camel2_time\": camel_times[1],\n",
    "        \"camel3_time\": camel_times[2],\n",
    "        \"camel4_time\": camel_times[3],\n",
    "        \"camel5_time\": camel_times[4],\n",
    "        \"camel_average\": camel_average,\n",
    "        \"kebab1_score\": kebab_scores[0],\n",
    "        \"kebab2_score\": kebab_scores[1],\n",
    "        \"kebab3_score\": kebab_scores[2],\n",
    "        \"kebab4_score\": kebab_scores[3],\n",
    "        \"kebab5_score\": kebab_scores[4],\n",
    "        \"kebab1_time\": kebab_times[0],\n",
    "        \"kebab2_time\": kebab_times[1],\n",
    "        \"kebab3_time\": kebab_times[2],\n",
    "        \"kebab4_time\": kebab_times[3],\n",
    "        \"kebab5_time\": kebab_times[4],\n",
    "        \"kebab_average\": kebab_average,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "all_participants_data_transformed = []\n",
    "\n",
    "directory_path = './data/'\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "        # Load data from the file\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # Transform data for each participant in the file\n",
    "        participants_data_transformed = [transform_participant_data(data)]\n",
    "\n",
    "        # Append transformed data to the list\n",
    "        all_participants_data_transformed.extend(participants_data_transformed)\n",
    "\n",
    "df = pd.DataFrame(all_participants_data_transformed)\n",
    "\n",
    "df.head()\n",
    "\n",
    "# CSV file path\n",
    "csv_file_path = \"./output.csv\"\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Data has been saved to {csv_file_path}\")\n",
    "\n",
    "csv_data_string = df.to_string(index=False)\n",
    "\n",
    "print(csv_data_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'output.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_df = pd.melt(df, id_vars=['participantID', 'experience'], var_name='question_id', value_name='value')\n",
    "\n",
    "melted_df[['question_type', 'question_number']] = melted_df['question_id'].str.extract('([a-zA-Z]+)(\\d+)', expand=True)\n",
    "\n",
    "melted_df['question_number'] = pd.to_numeric(melted_df['question_number'])\n",
    "\n",
    "# List of columns to exclude\n",
    "exclude_columns = ['camel1_score', 'camel2_score', 'camel3_score', 'camel4_score', 'camel5_score',\n",
    "                   'kebab1_score', 'kebab2_score', 'kebab3_score', 'kebab4_score', 'kebab5_score',\n",
    "                   'camel_average', 'kebab_average']\n",
    "\n",
    "# Filter columns to include only time-related columns\n",
    "time_df = melted_df[melted_df['question_type'].isin(['camel', 'kebab']) & ~melted_df['question_id'].isin(exclude_columns)]\n",
    "\n",
    "# Plot: Average time to answer questions vs. question id\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.barplot(x='question_id', y='value', data=time_df, hue='question_type', errorbar=None)\n",
    "plt.title('Average Time to Answer by Question IDs')\n",
    "plt.xlabel('Question IDs')\n",
    "plt.ylabel('Average Time to Answer in milliseconds')\n",
    "# plt.xticks(rotation=45, ha='right') \n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles, labels=['camelCase', 'kebab-case'], title='Question Case Format')\n",
    "\n",
    "ax.set_xticklabels(['camel1', 'camel2', 'camel3', 'camel4', 'camel5', 'kebab1', 'kebab2', 'kebab3', 'kebab4', 'kebab5'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_df = pd.melt(df, id_vars=['participantID', 'experience'], var_name='question_id', value_name='value')\n",
    "\n",
    "melted_df[['question_type', 'question_id_number']] = melted_df['question_id'].str.extract('([a-zA-Z]+)(\\d+)', expand=True)\n",
    "\n",
    "melted_df['question_id_number'] = pd.to_numeric(melted_df['question_id_number'], errors='coerce')\n",
    "\n",
    "score_df = melted_df[melted_df['question_id'].str.contains('_score')]\n",
    "\n",
    "# Group by question_id and calculate the average accuracy as a percentage\n",
    "avg_accuracy_df = score_df.groupby(['question_type', 'question_id'])['value'].mean() * 100\n",
    "avg_accuracy_df = avg_accuracy_df.reset_index()\n",
    "\n",
    "# Sort columns in ascending order\n",
    "avg_accuracy_df = avg_accuracy_df.sort_values(by=['question_id'])\n",
    "\n",
    "# Plot: Average Accuracy vs. question id\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.scatterplot(x='question_id', y='value', data=avg_accuracy_df, hue='question_type')\n",
    "# plt.title('Average Accuracy vs. Question IDs')\n",
    "plt.title('Average Accuracy Score for Different Question Types and IDs')\n",
    "plt.xlabel('Question IDs')\n",
    "plt.ylabel('Average Accuracy (%)')\n",
    "# plt.xticks(rotation=45, ha='right') \n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles, labels=['camelCase', 'kebab-case'], title='Question Case Format')\n",
    "# score\n",
    "ax.set_xticklabels(['camel1', 'camel2', 'camel3', 'camel4', 'camel5', 'kebab1', 'kebab2', 'kebab3', 'kebab4', 'kebab5'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_df = pd.melt(df, id_vars=['participantID', 'experience'], var_name='question_id', value_name='value')\n",
    "\n",
    "melted_df[['question_type', 'question_id_number']] = melted_df['question_id'].str.extract('([a-zA-Z]+)(\\d+)', expand=True)\n",
    "\n",
    "melted_df['question_id_number'] = pd.to_numeric(melted_df['question_id_number'], errors='coerce')\n",
    "\n",
    "score_df = melted_df[melted_df['question_id'].str.contains('_score')]\n",
    "\n",
    "avg_accuracy_df = score_df.groupby(['question_type', 'question_id'])['value'].agg(['mean', 'std']) * 100\n",
    "avg_accuracy_df = avg_accuracy_df.reset_index()\n",
    "\n",
    "avg_accuracy_df = avg_accuracy_df.sort_values(by=['question_id'])\n",
    "\n",
    "# Plot: Scatter Plot with Error Bars\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.scatterplot(x='question_id', y='mean', data=avg_accuracy_df, hue='question_type')\n",
    "# plt.errorbar(x=avg_accuracy_df['question_id'], y=avg_accuracy_df['mean'], yerr=avg_accuracy_df['std'], fmt='none', color='black', capsize=3)\n",
    "# plt.title('Scatter Plot of Accuracy with Error Bars')\n",
    "# plt.xlabel('Question ID')\n",
    "# plt.ylabel('Average Accuracy (%)')\n",
    "# plt.legend(title='Question Type')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.scatterplot(x='question_id', y='mean', data=avg_accuracy_df, hue='question_type', marker='o')\n",
    "\n",
    "lower_bound = avg_accuracy_df['mean'] - avg_accuracy_df['std']\n",
    "upper_bound = avg_accuracy_df['mean'] + avg_accuracy_df['std']\n",
    "\n",
    "# Draw error bars\n",
    "plt.errorbar(x=avg_accuracy_df['question_id'], y=avg_accuracy_df['mean'], yerr=[avg_accuracy_df['mean'] - lower_bound, upper_bound - avg_accuracy_df['mean']], fmt='none', color='gray', capsize=5)\n",
    "\n",
    "plt.title('Distribution of Accuracy by Question IDs')\n",
    "plt.xlabel('Question IDs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "# plt.xticks(rotation=45, ha='right') \n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles, labels=['camelCase', 'kebab-case'], title='Question Case Format')\n",
    "\n",
    "ax.set_xticklabels(['camel1', 'camel2', 'camel3', 'camel4', 'camel5', 'kebab1', 'kebab2', 'kebab3', 'kebab4', 'kebab5'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter columns to include only score-related columns\n",
    "accuracy_columns = [col for col in df.columns if '_score' in col]\n",
    "\n",
    "accuracy_melted_df = pd.melt(df, id_vars=['participantID', 'experience'], value_vars=accuracy_columns, \n",
    "                             var_name='question_id', value_name='accuracy')\n",
    "\n",
    "# Separate question type and number\n",
    "accuracy_melted_df[['question_type', 'question_id_number']] = accuracy_melted_df['question_id'].str.extract('([a-zA-Z]+)(\\d+)', expand=True)\n",
    "\n",
    "# Convert question_id_number to numeric\n",
    "accuracy_melted_df['question_id_number'] = pd.to_numeric(accuracy_melted_df['question_id_number'], errors='coerce')\n",
    "\n",
    "# Group by question_type and calculate the overall average accuracy as a percentage with standard deviation\n",
    "overall_avg_accuracy_df = accuracy_melted_df.groupby('question_type')['accuracy'].agg(['mean', 'std']) * 100\n",
    "overall_avg_accuracy_df = overall_avg_accuracy_df.reset_index()\n",
    "\n",
    "# Plot: Two-column average accuracy by case vs. case\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x='question_type', y='mean', data=overall_avg_accuracy_df, ci=None)\n",
    "plt.errorbar(x=overall_avg_accuracy_df['question_type'], y=overall_avg_accuracy_df['mean'],\n",
    "             yerr=overall_avg_accuracy_df['std'], fmt='none', color='gray', capsize=5)\n",
    "\n",
    "plt.title('Average Accuracy by Case (camelCase vs. kebab-case)')\n",
    "plt.xlabel('Question Case Format')\n",
    "plt.ylabel('Average Accuracy (%)')\n",
    "\n",
    "ax.set_xticklabels(['camelCase', 'kebab-case'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named 'df'\n",
    "\n",
    "# Filter columns to include only score-related columns\n",
    "score_columns = [col for col in df.columns if '_score' in col]\n",
    "\n",
    "# Group by participant and experience, calculate mean duration and accuracy\n",
    "grouped_df = df.groupby(['participantID', 'experience'])[score_columns].mean()\n",
    "\n",
    "# Reset the index to make participantID and experience regular columns\n",
    "grouped_df = grouped_df.reset_index()\n",
    "\n",
    "# Calculate mean duration and accuracy for each participant\n",
    "duration_columns = [col for col in df.columns if '_time' in col]\n",
    "accuracy_columns = [col for col in df.columns if '_score' in col]\n",
    "\n",
    "grouped_df['mean_duration'] = df[duration_columns].mean(axis=1)\n",
    "grouped_df['mean_accuracy'] = df[accuracy_columns].mean(axis=1) * 100\n",
    "\n",
    "# Print the grouped DataFrame to check the result\n",
    "print(\"Grouped DataFrame:\")\n",
    "print(grouped_df.head())\n",
    "\n",
    "# Create a scatter plot\n",
    "ax = sns.scatterplot(x='mean_duration', y='mean_accuracy', hue='experience', data=grouped_df)\n",
    "plt.title('Scatter Plot of Duration vs Accuracy')\n",
    "plt.xlabel('Mean Duration (in milliseconds)')\n",
    "plt.ylabel('Mean Accuracy (%)')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles, labels=['No experience', '3+ years', 'Bachelor INF student'], title='Participant experience')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'participantID': ['24e6c294', 'f016b9fd', '1f29dc05', 'b58ce07a', '9d8a6f15',\n",
    "                       '0562b141', 'd3d7734f', '97842511', '93de7372', '8792e96d'],\n",
    "    'experience': ['bachelor-inf', 'no-experience', 'more-three', 'bachelor-inf', 'more-three',\n",
    "                    'no-experience', 'bachelor-inf', 'bachelor-inf', 'more-three', 'more-three'],\n",
    "    'camel1_score': [1, 1, 0, 1, 1, 1, 1, 0, 1, 1],\n",
    "    'camel2_score': [0, 1, 0, 0, 1, 0, 0, 1, 1, 1],\n",
    "    'camel3_score': [1, 1, 1, 1, 1, 0, 1, 1, 1, 1],\n",
    "    'camel4_score': [1, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    'camel5_score': [1, 1, 1, 0, 1, 1, 1, 1, 1, 1],\n",
    "    'mean_duration': [1676.1, 7248.9, 3145.2, 3053.2, 8474.7, 4707.1, 6152.0, 3050.9, 2966.4, 2330.4],\n",
    "    'mean_accuracy': [90, 90, 80, 80, 100, 70, 90, 90, 100, 90],\n",
    "    'kebab1_score': [1, 1, 1, 1, 1, 1, 1, 0, 1, 1],\n",
    "    'kebab2_score': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    'kebab3_score': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    'kebab4_score': [1, 1, 1, 1, 1, 1, 0, 1, 1, 1],\n",
    "    'kebab5_score': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    'kebab1_time': [2897.6, 7056.1, 4537.4, 2242.7, 4122.8, 3600.2, 6216.1, 2239.6, 2599.6, 2122.4],\n",
    "    'kebab2_time': [4573.2, 13175.3, 5895.6, 3250.7, 9900.7, 10381.6, 10638.2, 7156.8, 4915.8, 4712.6],\n",
    "    'kebab3_time': [3991.4, 8182.6, 6474.8, 3295.0, 4999.5, 7525.4, 4173.6, 6140.4, 4483.5, 4195.3],\n",
    "    'kebab4_time': [4187.9, 15760.3, 6025.434562, 2613.4, 4995.1, 4267.2, 27065.0, 3467.7, 2399.6, 2792.4],\n",
    "    'kebab5_time': [8365.5, 11522.7, 3901.6, 3467.1, 5381.1, 4111.0, 8177.1, 2569.5, 3583.1, 2143.3],\n",
    "    'kebab_average': [4803.12, 11139.4, 5366.966912, 2973.78, 5879.840004, 5977.08, 11254.0, 4314.8, 3596.32, 3193.2]\n",
    "})\n",
    "\n",
    "# Extract experience groups\n",
    "groups = df.groupby('experience')\n",
    "\n",
    "# Display the five-number summary, mean, and standard deviation\n",
    "for name, group in groups:\n",
    "    print(f\"\\nSummary for {name}:\")\n",
    "    print(group[['mean_duration', 'mean_accuracy']].describe(percentiles=[.25, .5, .75]))\n",
    "    print(f\"\\nMean for {name}:\")\n",
    "    print(group[['mean_duration', 'mean_accuracy']].mean())\n",
    "    print(f\"\\nStandard Deviation for {name}:\")\n",
    "    print(group[['mean_duration', 'mean_accuracy']].std())\n",
    "    \n",
    "    \n",
    "summary_data = []\n",
    "\n",
    "for name, group in groups:\n",
    "    summary = group[['mean_duration', 'mean_accuracy']].describe(percentiles=[.25, .5, .75])\n",
    "    mean = group[['mean_duration', 'mean_accuracy']].mean()\n",
    "    std_dev = group[['mean_duration', 'mean_accuracy']].std()\n",
    "\n",
    "    summary_data.append({\n",
    "        'Experience': name,\n",
    "        'Summary': summary.to_string(),\n",
    "        'Mean': mean.to_string(),\n",
    "        'Standard_Deviation': std_dev.to_string()\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "summary_df.to_csv('summary_statistics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "group1 = df[df['experience'] == 'bachelor-inf']['mean_duration']\n",
    "group2 = df[df['experience'] == 'no-experience']['mean_duration']\n",
    "\n",
    "t_statistic, p_value = ttest_ind(group1, group2, nan_policy='omit')\n",
    "\n",
    "print(f'T-statistic: {t_statistic}')\n",
    "print(f'P-value: {p_value}')\n",
    "\n",
    "# Check the significance level\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print('Reject the null hypothesis. There is a significant difference between the groups.')\n",
    "else:\n",
    "    print('Fail to reject the null hypothesis. There is no significant difference between the groups.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "import scipy.stats as stats\n",
    "\n",
    "file_path = 'output.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "# print(df)\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Separate the data for camelCase and kebab-case\n",
    "camel_data = df[['camel1_score', 'camel2_score', 'camel3_score', 'camel4_score', 'camel5_score',\n",
    "                 'camel1_time', 'camel2_time', 'camel3_time', 'camel4_time', 'camel5_time']]\n",
    "\n",
    "kebab_data = df[['kebab1_score', 'kebab2_score', 'kebab3_score', 'kebab4_score', 'kebab5_score',\n",
    "                 'kebab1_time', 'kebab2_time', 'kebab3_time', 'kebab4_time', 'kebab5_time']]\n",
    "\n",
    "\n",
    "def perform_t_tests(camel, kebab, variable):\n",
    "    statistic, p_value = stats.ttest_ind(camel, kebab)\n",
    "    # statistic, p_value = stats.ttest_rel(camel, kebab)\n",
    "    print(f\"\\nT-Test for {variable}:\")\n",
    "    print(f\"  Statistic: {statistic}\")\n",
    "    print(f\"  P-Value: {p_value}\\n\")\n",
    "\n",
    "for column in camel_data.columns:\n",
    "    \n",
    "    kebab_column = column.replace('camel', 'kebab')\n",
    "    \n",
    "    if kebab_column not in kebab_data.columns:\n",
    "        kebab_column = column.replace('mean_', 'kebab_')\n",
    "    \n",
    "    perform_t_tests(camel_data[column], kebab_data[kebab_column], column)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
